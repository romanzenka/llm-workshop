{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/argonne-lcf/llm-workshop/blob/main/RAGTutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jGpjFGh2qX2O"
      },
      "source": [
        "#Resource Augmented Generation (RAG)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5YjAA14ih8R"
      },
      "source": [
        "#### Imagine you went to live under a rock on August 2006. When you come out in 2024, you are asked how many planets revolve around the sun. What would you say?...\n",
        "![pluto](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/pluto_planets.jpeg?raw=1)\n",
        "\n",
        "This is similar to LLMs which are trained with data until a certain point and then asked questions on data they are not trained on. Understandably, LLMs will either be unable to answer or simply hallucinate a probably wrong answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W3HmhY-rl29V"
      },
      "source": [
        "###What can be done?\n",
        "\n",
        "Have the LLM go to the library using **Research Augmented Generation (RAG)**!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PXtqMcK1nAv_"
      },
      "source": [
        "RAG involves adding your own data (via a retrieval tool) to the prompt that you pass into a large language model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAGt5yQVrz0C"
      },
      "source": [
        "![rag architecture](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/rag-overview.original.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y6N4A87lt3Wq"
      },
      "source": [
        "RAG has been shown to improve LLM prediction accuracy without needing to increase parameter size.\n",
        "\n",
        "![rag architecture](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/rag_acc_v_size.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDcwB4BMyUQ5"
      },
      "source": [
        "RAG also increases explainability by giving the source for information.\n",
        "\n",
        "![rag architecture](https://github.com/architvasan/LLMWorkshop/blob/main/rag_images/rag_source_locator.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s42Iqfmjy-1E"
      },
      "source": [
        "#Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rVkTgnU80jjV"
      },
      "source": [
        "### 1. Install + load relevant modules:\n",
        "*   langchain\n",
        "*   torch\n",
        "*   transformers\n",
        "*   sentence-transformers\n",
        "*   datasets\n",
        "*   faiss-cpu  \n",
        "*   pypdf\n",
        "*  unstructure[pdf]\n",
        "*  huggingface_hub (add hf_token)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1uDuyDLd1Xi"
      },
      "outputs": [],
      "source": [
        "!pip install langchain==0.1.5\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install pypdf\n",
        "!pip install sentence-transformers\n",
        "!pip install unstructured\n",
        "!pip install unstructured[pdf]\n",
        "!pip install tiktoken\n",
        "!pip install huggingface_hub\n",
        "from huggingface_hub import login\n",
        "\n",
        "with open('/homes/ac.rzenka/llm/.token', 'r') as file:\n",
        "    hf_token = file.read().replace('\\n', '')\n",
        "\n",
        "login(token=hf_token, add_to_git_credential=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ht5HpwXiRXAO"
      },
      "source": [
        "What is LangChain?\n",
        "\n",
        "LangChain is a framework for developing applications powered by language models. It enables:\n",
        "1. Language model importing\n",
        "2. Prompt templating\n",
        "3. Chains:They combine LLMs with other components, creating applications by executing a sequence of functions.\n",
        "4. Document Loading\n",
        "5. Text splitting\n",
        "6. Retrieval"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5agp6iRm-7jD"
      },
      "source": [
        "### 2. Choose a dataset to use and then load it into your code\n",
        "Here we are using the pdfs loaded in pdfs/. We load this using langchain DirectoryLoader."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vBx0cQJ9Jmtv"
      },
      "source": [
        "We can load multiple types of datasets into this example though the most commonly used are PDFs and websites.\n",
        "\n",
        "To load websites, we could also use `langchain WebBaseLoader`\n",
        "\n",
        "In this example, we will consider PDFs and load them in using `langchain DirectoryLoader`.\n",
        "\n",
        "We host all PDFs at the PDFs directory `llm-workshop/tutorials/04-rag/PDFs`\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8YpJK93pNsZ"
      },
      "outputs": [],
      "source": [
        "! git clone https://github.com/argonne-lcf/llm-workshop.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YYoEp4FJz6oK"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 7/7 [00:44<00:00,  6.43s/it]\n"
          ]
        }
      ],
      "source": [
        "from langchain.document_loaders import DirectoryLoader\n",
        "loader = DirectoryLoader('PDFs', glob=\"**/*.pdf\", show_progress=True)\n",
        "documents = loader.load()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjvyEOMMBB7G"
      },
      "source": [
        "### 3. Now, we need to split our documents into chunks.\n",
        "We want the embedding to be greater than 1 word but much less than an entire page. There are different ways to do this:\n",
        "\n",
        "\n",
        "*  Recursive: Recursively splits text. Useful for keeping related pieces of text next to each other.\n",
        "*   HTML: Splits text based on HTML-specific characters.\n",
        "*   Markdown: Splits on Markdown-specific characters\n",
        "*   Code: Splits text based on characters specific to coding languages.\n",
        "*   Token: Splits text on tokens. Can chunk tokens together\n",
        "*   Character: Splits based on some user defined character.\n",
        "\n",
        "Here we use recursive where the dataset is split using a set of characters. The default characters provided to it are [\"\\n\\n\", \"\\n\", \" \", \"\"].  A large text is split by the first character \\n\\n. If the first split by \\n\\n is still large then it moves to the next character which is \\n and tries to split by it. This continues until the chunk size is reached.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfgRcG6g0XOG"
      },
      "outputs": [],
      "source": [
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)\n",
        "docs = text_splitter.split_documents(documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyTRRvU6DboO"
      },
      "source": [
        "### 4. Then we embed the chunked texts using a Transformer.\n",
        "This allows us to encode the text into our search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXny33dcCHZc"
      },
      "source": [
        "Embedding converts text to a numerical representation in a vector space. RAG compares the embeddings of user queries within the vector of the knowledge library.\n",
        "\n",
        "In this example, we choose a simple embedding using the MiniLM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kabjUo1I1Pev"
      },
      "outputs": [],
      "source": [
        "from langchain.embeddings import HuggingFaceEmbeddings\n",
        "modelPath = \"sentence-transformers/all-MiniLM-l6-v2\"\n",
        "model_kwargs = {'device':'cpu'}\n",
        "encode_kwargs = {'normalize_embeddings':False}\n",
        "embeddings = HuggingFaceEmbeddings(\n",
        "  model_name = modelPath,\n",
        "  model_kwargs = model_kwargs,\n",
        "  encode_kwargs=encode_kwargs\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvkGZSJvEFeK"
      },
      "source": [
        "### 5.Create a vector database\n",
        "Vector databases, also called vector storage, efficiently store and retrieve vector data, which are arrays of numerical values representing points in multi-dimensional space. They're useful for handling data like embeddings from deep learning models or numerical features. Unlike traditional relational databases, which aren't optimized for vectors, vector databases offer efficient storage, indexing, and querying for high-dimensional and variable-length vectors.\n",
        "\n",
        " Here, we build this using the FAISS utility."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqccXGwoEYP-"
      },
      "source": [
        "![vector_database](https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/04-rag/rag_images/vector_database.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nMw4wTnQvaM"
      },
      "source": [
        "Some key features of vector databases include:\n",
        "\n",
        "1. Indexing: Techniques such as k-d trees, ball trees, or locality-sensitive hashing (LSH), enabling fast and efficient search operations over large datasets of vectors.\n",
        "\n",
        "2. Similarity Search: Given a query vector, the database can efficiently find the closest vectors in the dataset based on a chosen distance metric (e.g., Euclidean distance or cosine similarity).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aWu5YGPj1e6B",
        "outputId": "f1d5e05c-aa10-4e46-c599-7c22ed37ce86"
      },
      "outputs": [],
      "source": [
        "from langchain.vectorstores import FAISS\n",
        "db = FAISS.from_documents(docs, embeddings)\n",
        "question = \"What is RF Fold?\"\n",
        "searchDocs = db.similarity_search(question)\n",
        "print(searchDocs[0].page_content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5A56JddAEkCF"
      },
      "source": [
        "### 6. Initialize the LLM that will be used for question answering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdUokERIMnz3"
      },
      "source": [
        "Here, we use a pretrained model flan-t5-large as part of a HuggingFacePipeline. This will later be chained with the vector database for RAG."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9CXf3N6c134r"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM,pipeline\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-large\")\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-large\")\n",
        "pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
        "llm = HuggingFacePipeline(\n",
        "   pipeline = pipe,\n",
        "   model_kwargs={\"temperature\": 0, \"max_length\": 2048},\n",
        ")\n",
        "#'HuggingFaceH4/zephyr-7b-beta'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvJhhDzYEvhu"
      },
      "source": [
        "### 7. Retrieve data and use it to answer a question"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QOQG22pHOf5W"
      },
      "source": [
        "![rag_workflow](https://github.com/argonne-lcf/llm-workshop/blob/main/tutorials/04-rag/rag_images/rag_workflow.png?raw=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWJHxBc9hzJB"
      },
      "source": [
        "Let's ask questions it would only be able to know if the model actually read the texts!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nMrN1_3B2cA1"
      },
      "outputs": [],
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "template = \"\"\"Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer. Keep the answer as concise as possible.\n",
        "{context}\n",
        "Question: {question}\n",
        "Helpful Answer:\"\"\"\n",
        "QA_CHAIN_PROMPT = PromptTemplate.from_template(template)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HLCz3eiX2e0g",
        "outputId": "211a3750-3a75-4707-e349-18e1606b1a1f"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "  llm=llm,\n",
        "  chain_type=\"stuff\",\n",
        "  retriever=db.as_retriever(),\n",
        "  chain_type_kwargs={\"prompt\": QA_CHAIN_PROMPT}\n",
        ")\n",
        "result = qa_chain ({ \"query\" : \"What technique proposed in 2023 can be used to predict protein folding?\" })\n",
        "print(result[\"result\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynkOkS39K2FT"
      },
      "source": [
        "Now let's ask the chain where to find the article related to RFDiffusion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWP0ZJ0Cc7uw",
        "outputId": "6fb5f0f0-7dfc-4f47-cecf-3811454236ec"
      },
      "outputs": [],
      "source": [
        "qa_chain ({ \"query\" : \"Which scientific article should I read to learn about RFdiffusion for protein folding?\" })"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
